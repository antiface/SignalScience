“System dynamics of human signalling behaviour”
 
     22-11-2009 / 9h09
 
1. Animal behaviour
 a) Animal communication
b) Signalling theory
c) Bargaining theory


2. Human interaction
a) Microeconomics
 b) Telecommunications
 
3. Technology
a) Ethnography of Technology / the Industrial
b) Ethics of Information
 
Man & Tools
Man & Information
Man & Behavior
Man & Communication
 
4. The Self & Others
 a) Ethics
b) The Dialogical Self
 
Conclusion: An Ethics of Communication + “Designing for Trust” + “Assurance Mechanisms”
 
Keep a tab on Pings: Pings are assigned a value, a 1 or a 0, on or off, good-connector / bad-connector
 
1. Animal behaviour
2. Animal communication
3. Human interaction
4. Human communication (symbolic interaction + signalling, i.e. signal-based interaction)
5. Man & Information Technology
 
I. Security
 II. Good behaviour (Cooperation vs. Control)
 III. Policies & Sanctions
 
Transgression control
Trust
Explicit contracting
Agreements, bargaining
Game theory & telecommunications systems
Screen, yes, but use SIGNALLING METHODS of legitimatizing
EVERY PING = is assigned a value, a “cost” or “price”
True costs can be unbeknownst to users
Compute costs, compute for honesty of signals
How can you tell if a ping is an honest signal?
Evolutionarily stable strategies + Reputation management systems + civic virtues + schedules of reinforcement
 
1. Understand the stakes;
2. Understand the human behaviours involved (the system’s “behavioural ecology”);
3. Implement control system at the outset
4. Control for abuses
 
I. Reinforcement = only exemplary to point out something about human behaviour;
 II. That humans unless given adequate incentives, positive or negative, will behave most selfishly;
 III. BUT = all selfish agents want to appear to others as being in good standing, as having a good reputation and being just and honest and esp. trustworthy!
 
IV. Trust from a game-theoretic perspective is as follows:
 a) All strategies in a multi-player game are interdependent on what each player knows about actual moves played and about other players’ knowledge;
 b) If you always knew the other players’ next moves, you’d always win, you’d always be ahead of the game;
 c) Absolute trust is knowing that the other players will always cooperate and always respect agreements;
 d) Perfect trust is the same as an extreme explicit contract with great sanctions: Except that it is automatic and costs nothing to implement and maintain, and it is called a CONSCIENCE, which is itself a call to moral civic action.
 
If we knew that people would selfishly seek to be truly virtuous, then we wouldn’t need policies and laws. But people seek selfishly to bend the rules, to commit transgressions, great or small. It all depends on the need at hand. It is not inherent in humans that they will transgress. They need a motive. Therefore, the best trust system is a system where there is no motive to commit to wrongful action. Give them ample reason to do something else, something other than wrongful action and you will have a perfect trust system. Look at athletes in major sports: They will not commit transgressions that risk putting their team in danger or the integrity or dignity of other players on their team. In Hockey, individuals will bend the rules, will make illegal moves, but never at the expense of putting a banner of shame over the name of the coach and team.
 
One can take a top-down approach of explicit contracts and heavy policies / policing with costly sanctions, but what else?
As “Network Command”, you cannot trust that all players will implicitly cooperate; you in fact know the following: that there will always be some players that will want to “free-ride”, for where there are co-operators, there are always some free-riders;
How do you minimize the risks involved with selfish participants?
Answer: You make pings costly.
If making a good or bad ping – regardless! – is costly, this will naturally minimize bad pings.
If making an honest signal is costly, only those who can afford honest signals will make them.
A risky bet on being honest is always a certain win;
So even if bad pings are NOT costly and even if you encourage foul play, then you will still know who the honest players are
Make one’s own reputation invisible to oneself but visible to everyone else, + make all your moves visible to other players, every last ping
+ put a picture of each individual on his/her profile
Make it morally shameful to commit transgressions, the human conscience is the greatest control mechanism, it is the surest guide to right / rightful action
You can have a perfect trust system but the problem remains largely unformulated, How do you market for a perfect trust system?
 
I. Honest signals (basis for signalling theory);
II. Skinner’s schedules of reinforcement;
III. See “Network-as-Skinner-Box”, that is, the Network as “learning-machine”; like a rat’s labyrinth.
 
Make a tree diagram of possible moves (pings) a user can make, + work at the NODES. Give incentives at each node to compel right moves;
Give them the freedom to misbehave, to go bankrupt;
Secretly don’t let them do themselves any harm;
State that you have a perfect trust system = ask people “If you think you can beat the system, just try,”;
People have to click on a big button that says “try to beat the system” which is linked to a trigger mechanism;
In fact, use triggers for all bad moves, different triggers for different values, for different “costs / prices”;
Instance of a trigger: When making a “bad” move, a signal is sent, “The system is presently busy computing your last move,” + then make them wait indefinitely. When in doubt, the system shuts down FOR THAT USER, and a little clock appears that turns and turns and turns forever…
People, rather than do something bad, will smash their own computers to smithereens.
 
10h08
 
Look at a user’s goals as you would the evolutionary goals of an organism in an ecosystem. The organism we would say, seeks to attain an optimum in terms of fitness, right? So whatever value or goal that you can assign to a user, whether it is the quest for money, power, or fame, or just the desire to be a prankster or an ass, if you know a goal, any goal, you can screen for that goal by looking at the user’s signals. Like a macaque whose erection gives away his desire to copulate, a user with the desire to commit an abuse within the system will give it away through his behaviour.
 
How do we do this? You look at every goal as part of an evolutionary system. The user’s goals are all goals because they make him fitter. That is, if he seeks to cooperate, he seeks so to augment his fitness, the same going for users who seek to transgress or commit to abuses. The difference here is in the criteria for fitness, the very concept of fitness here merely being whatever is an incentive.
 
We know that there are limited reasons why someone would waste time on a system. Mostly it is because whatever they are doing is passing the time and is at least remotely entertaining, has something playful about it. We can look at Eric Berne’s model of human behaviour in transactional analysis. He saw men as being equipped with one major need: To behave to structure time. The rest were just manners in which men structured time. They structured time with work, hobbies, or games. You can screen for games people are playing. Anything that has a strongly formal nature can be screened for. You just have to observe the pattern somewhere. And every intention and goal that someone has within the network, it will have its own “signature”, like a chemical trace, or like a behaviour in the evolutionary history of an organism, as an ethologist studies in a given animal.
 
It’s important to understand how signalling systems have evolved over time.
 
Signals vs. Cues = a signal is the bird warning its predator that the predator has been spotted, it signals “stop predating me”, and a cue is the sound a small animal makes inadvertently that gives a cue to its predator that prey is near.
 
How to make users use the network intensely without some shiny, catchy game-like thing? You want to sell intense use based on a powerful trust system?
 
11h16 – One can already pretty must predict the evolution of behaviours within this network once it has been officially launched. We know that there will be behaviours. We know that there will be divergent behaviours. We know that there will be material constraints and that each user’s interaction with the other users will influence his behaviour. We know that each user is going to be attempting, for selfish reasons, to mind his own self-interest first and foremost and in the purest senses. We know that the behaviour of others, and therefore any information about the behaviour of others, will exert a powerful influence on the individual user. So already we know that behaviours will evolve that will be both constrained by physical limitations within the system, also by the limits of physical impossibility, as well as evolving in the interaction of the individual and the others in the network.
 
We know from this that any behaviour that is not only selfish but is also an abuse, that there will tend to be an equilibrium which sets in at the limits of what one can get away with, that is, that all abusers within the system will try to get away with it by not being seen. If other users can see my behaviours, they can see that I am abusing the system. All users will want to have a clean slate, a perfect reputation. An evolutionarily stable strategy of cooperation and fair play will necessarily set in.
 
The reason that abusive uses occur in networks is that people are capitalizing from the use of time-wasting games and other such applications. People will naturally use the system and use it intensely. You don’t have to do anything. What causes abusive network usage is linked directly to the foul play or wrongdoing of the network administrators. But you also cannot predict the possible outcomes of emergent properties in your network. You must merely be ready to spot them and take advantage of them. Start the damn thing now and just fix it as you go along. Even Google and eBay shut down their systems in order to execute rather ordinary measures of network and hardware, etc., maintenance.
 
See: Skinner, Resistance to Change, Strength of Operant
