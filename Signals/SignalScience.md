Signal Science 002
==================

What is a Signal?

The universe is made of signals.

The universe is made of computations.

Signals are time series.

Signals are physical events in the world.

A signal "is a function that conveys information about the behavior or attributes of some phenomenon". (Wikipedia)

In the physical world, any quantity exhibiting variation in time or variation in space (such as an image) is potentially a signal that might provide information on the status of a physical system, or convey a message between observers, among other possibilities.

A signal is a physical quantity which varies with respect to time,space & contain information from source to destination.

In electronics, noise is a random fluctuation in an electrical signal.

In communication systems, the noise is an error or undesired random disturbance of a useful information signal, introduced before or after the detector and decoder. The noise is a summation of unwanted or disturbing energy from natural and sometimes man-made sources.

Information theory is a branch of applied mathematics, electrical engineering, and computer science involving the quantification of information. 

A key measure of information is known as entropy, which is usually expressed by the average number of bits needed to store or communicate one symbol in a message. Entropy quantifies the uncertainty involved in predicting the value of a random variable. For example, specifying the outcome of a fair coin flip (two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (six equally likely outcomes).

In information theory, entropy is a measure of the uncertainty associated with a random variable.

In this context, the term usually refers to the Shannon entropy, which quantifies the expected value of the information contained in a message.

Equivalently, the Shannon entropy is a measure of the average information content one is missing when one does not know the value of the random variable.

Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'. The entropy rate of a data source means the average number of bits per symbol needed to encode it.

From the preceding example, note the following points:
* The amount of entropy is not always an integer number of bits.
* Many data bits may not convey information. For example, data structures often store information redundantly, or have identical sections regardless of the information in the data structure.

Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits

Information, in its most restricted technical sense, is a sequence of symbols that can be interpreted as a message. Information can be recorded as signs, or transmitted as signals. Information is any kind of event that affects the state of a dynamic system. Conceptually, information is the message (utterance or expression) being conveyed. The meaning of this concept varies in different contexts. Moreover, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, understanding, mental stimuli, pattern, perception, representation, and entropy.

From the stance of information theory, information is taken as a sequence of symbols from an alphabet, say an input alphabet χ, and an output alphabet ϒ. Information processing consists of an input-output function that maps any input sequence from χ into an output sequence from ϒ. The mapping may be probabilistic or determinate. It may have memory or be memoryless.[

The cognitive scientist and applied mathematician Ronaldo Vigo argues that information is a relative concept that involves at least two related entities in order to make quantitative sense. These are: any dimensionally defined category of objects S, and any of its subsets R. R, in essence, is a representation of S, or, in other words, carries or conveys representational (and hence, conceptual) information about S. Vigo then defines the amount of information that R conveys about S as the rate of change in the complexity of S whenever the objects in R are removed from S. Under "Vigo information", pattern, invariance, complexity, representation, and information—five fundamental constructs of universal science—are unified under a novel mathematical framework. Among other things, the framework aims to overcome the limitations of Shannon-Weaver information when attempting to characterize and measure subjective information.

The term information content is used to refer to the meaning of information as opposed to the form or carrier of the information. For example, the meaning that is conveyed in an expression (which may be a proposition) or document, which can be distinguished from the sounds or symbols or codes and carrier that physically form the expression or document. An information content is composed of a propositional content and an illocutionary force.

In telecommunication, signaling (or signalling in British English) has the following meanings:
* the use of signals for controlling communications
* the information exchange concerning the establishment and control of a telecommunication circuit and the management of the network, in contrast to user information transfer
* the sending of a signal from the transmitting end of a telecommunication circuit to inform a user at the receiving end that a message is to be sent.

--------------------------------------------------------

Signal Science 003
==================

A message in its most general meaning is an object of communication. It is a vessel which provides information. Yet, it can also be this information. Therefore, its meaning is dependent upon the context in which it is used; the term may apply to both the information and its form.

More precisely, in communications science, a message is information which is sent from a source to a receiver. Some common definitions include:
* Any thought or idea expressed in a language, prepared in a form suitable for transmission by any means of communication.
* An arbitrary amount of information whose beginning and end are defined or implied.

In communication between humans, messages can be verbal or nonverbal:
* A verbal message is an exchange of information using words. Examples include face-to-face communication, telephone calls, voicemails, etc.
* A nonverbal message is communicated through actions or behaviors rather than words. Examples include the use of body language and the actions made by an individual idea.

In economics, more precisely in contract theory, signalling is the idea that one party (termed the agent) credibly conveys some information about itself to another party (the principal). For example, in Michael Spence's job-market signalling model, (potential) employees send a signal about their ability level to the employer by acquiring certain education credentials. The informational value of the credential comes from the fact that the employer assumes it is positively correlated with having greater ability.

Signalling took root in the idea of asymmetric information (a deviation from perfect information), which says that in some economic transactions, inequalities in access to information upset the normal market for the exchange of goods and services. In his seminal 1973 article, Michael Spence proposed that two parties could get around the problem of asymmetric information by having one party send a signal that would reveal some piece of relevant information to the other party. That party would then interpret the signal and adjust her purchasing behaviour accordingly — usually by offering a higher price than if she had not received the signal.

There are, of course, many problems that these parties would immediately run into.
* How much time, energy, or money should the sender (agent) spend on sending the signal?
* How can the receiver (the principal, who is usually the buyer in the transaction) trust the signal to be an honest declaration of information?
* Assuming there is a signalling equilibrium under which the sender signals honestly and the receiver trusts that information, under what circumstances will that equilibrium break down?

Within evolutionary biology, signalling theory is a body of theoretical work examining communication between individuals. The central question is when organisms with conflicting interests should be expected to communicate "honestly". Mathematical models in which organisms signal their condition to other individuals as part of an evolutionarily stable strategy are the principal form of research in this field.

In biology, signals are traits, including structures and behaviors, that have evolved specifically because they change the behavior of receivers in ways that benefit the signaller. Traits or actions that benefit the receiver exclusively are called cues. When an alert bird deliberately gives a warning call to a stalking predator and the predator gives up the hunt, the sound is a signal. When a foraging bird inadvertently makes a rustling sound in the leaves that attracts predators and increases the risk of predation, the sound is a 'cue'.

Signalling systems are shaped by the extent to which signallers and receivers have mutual interests. An alert bird warning off a stalking predator is communicating something useful to the predator: that it has been detected by the prey; it might as well quit wasting its time stalking this alerted prey, which it is unlikely to catch. When the predator gives up, the signaller can get back to other important tasks. Once the stalking predator is detected, the signalling prey and receiving predator have a mutual interest terminating the hunt.

Because there are both mutual and conflicting interests in most animal signalling systems, the fundamental problem in evolutionary signalling games is dishonesty or cheating. Why don’t foraging birds just give warning calls all the time, at random (false alarms), just in case a predator is nearby? If peacocks with bigger tails are preferred by peahens, why don’t all peacocks display big tails? Too much cheating would disrupt the correlation at the foundation of the system, causing it to collapse. Receivers should ignore the signals if they are not useful to them and signallers shouldn’t invest in costly signals if they won’t alter the behavior of receivers in ways that benefit the signaller. What prevents cheating from destabilizing signalling systems? It might be apparent that the costs of displaying signals must be an important part of the answer. However, understanding how costs can stabilize an "honest" correlation between the public signal trait and the private signalled quality has turned out to be a long, interesting process. If many animals in a group send too many dishonest signals, then their entire signalling system will collapse, leading to much poorer fitness of the group as a whole. Every dishonest signal weakens the integrity of the signalling system, and thus weakens the fitness of the group.

Human behaviors may also serve as examples of costly signals. Evidence for costly signalling has been found in many areas of human interaction, including risk taking, hunting, and religion. In general, these signals provide information about a person’s phenotypic quality or cooperative tendencies.

Randomness means different things in various fields. Commonly, it means lack of pattern or predictability in events.

The Oxford English Dictionary defines "random" as "Having no definite aim or purpose; not sent or guided in a particular direction; made, done, occurring, etc., without method or conscious choice; haphazard." This concept of randomness suggests a non-order or non-coherence in a sequence of symbols or steps, such that there is no intelligible pattern or combination.

Applied usage in science, mathematics and statistics recognizes a lack of predictability when referring to randomness, but admits regularities in the occurrences of events whose outcomes are not certain. For example, when throwing two dice and counting the total, we can say that a sum of 7 will randomly occur twice as often as 4. This view, where randomness simply refers to situations where the certainty of the outcome is at issue, applies to concepts of chance, probability, and information entropy. In these situations, randomness implies a measure of uncertainty, and notions of haphazardness are irrelevant.

The fields of mathematics, probability, and statistics use formal definitions of randomness. In statistics, a random variable is an assignment of a numerical value to each possible outcome of an event space. This association facilitates the identification and the calculation of probabilities of the events. A random process is a sequence of random variables describing a process whose outcomes do not follow a deterministic pattern, but follow an evolution described by probability distributions. These and other constructs are extremely useful in the probability calculus.

In physics and cosmology, digital physics is a collection of theoretical perspectives based on the premise that the universe is, at heart, describable by information, and is therefore computable. Therefore, the universe can be conceived of as either the output of a computer program, a vast, digital computation device, or mathematically isomorphic to such a device.
Digital physics is grounded in one or more of the following hypotheses; listed in order of decreasing strength. The universe, or reality:
* is essentially informational (although not every informational ontology needs to be digital)
* is essentially computable
* can be described digitally
* is in essence digital
* is itself a computer
* is the output of a simulated reality exercise

Social entropy is a macrosociological systems theory. It is a measure of the natural decay within a social system. It can refer to the decomposition of social structure or of the disappearance of social distinctions. Much of the energy consumed by a social organization is spent to maintain its structure, counteracting social entropy, e.g., through legal institutions, education and even the promotion of television viewing. Anomie is the maximum state of social entropy[disputed – discuss]. Social Entropy implies the tendency of social networks and society in general to break down over time, moving from cooperation and advancement towards conflict and chaos.

Social Entropy and Energy Inputs

Modern Western complex societies remain organized by large inputs of energy to mitigate the natural progression of increasing entropy (disorder), according to the Second Law of Thermodynamics, a fundamental law of physics. This effectively states that the Entropy (disorder) of the universe as a whole increases with time. To create local order or complexity ultimately requires the expenditure of energy. As the system becomes more complex, through access to energy, it becomes more susceptible to changes that may occur if one were to remove this source of energy. Take away the energy inputs (largely from fossil fuels) and organization corrodes, thus society becomes less cohesive and trends toward anomie.

Energetics (also called energy economics) is the study of energy under transformation. Because energy flows at all scales, from the quantum level to the biosphere and cosmos, energetics is a very broad discipline, encompassing for example thermodynamics, chemistry, biological energetics, biochemistry and ecological energetics. Where each branch of energetics begins and ends is a topic of constant debate. For example, Lehninger (1973, p. 21) contended that when the science of thermodynamics deals with energy exchanges of all types, it can be called energetics.

In economics and contract theory, information asymmetry deals with the study of decisions in transactions where one party has more or better information than the other. This creates an imbalance of power in transactions which can sometimes cause the transactions to go awry, a kind of market failure in the worst case. Examples of this problem are adverse selection,[1] moral hazard, and information monopoly.[2] Most commonly, information asymmetries are studied in the context of principal–agent problems. Information asymmetry causes misinforming and is essential in every communication process.

----------------------------------------------------

Signal Science 004
==================

Functions of communication

While there are as many kinds of communication as there are kinds of social behaviour, a number of functions have been studied in particular detail. They include:

* agonistic interaction: everything to do with contests and aggression between individuals. Many species have distinctive threat displays that are made during competition over food, mates or territory; much bird song functions in this way. Often there is a matched submission display, which the threatened individual will make if it is acknowledging the social dominance of the threatener; this has the effect of terminating the aggressive episode and allowing the dominant animal unrestricted access to the resource in dispute. Some species also have affiliative displays which are made to indicate that a dominant animal accepts the presence of another.

* Mating rituals: signals made by members of one sex to attract or maintain the attention of potential mate, or to cement a pair bond. These frequently involve the display of body parts, body postures (gazelles assume characteristic poses as a signal to initiate mating), or the emission of scents or calls, that are unique to the species, thus allowing the individuals to avoid mating with members of another species which would be infertile. Animals that form lasting pair bonds often have symmetrical displays that they make to each other: famous examples are the mutual presentation of reeds by Great Crested Grebes, studied by Julian Huxley, the triumph displays shown by many species of geese and penguins on their nest sites and the spectacular courtship displays by birds of paradise and manakins.
ownership/territorial: signals used to claim or defend a territory, food, or a mate.

* Food-related signals: many animals make "food calls" that attract a mate, or offspring, or members of a social group generally to a food source. When parents are feeding offspring, the offspring often have begging responses (particularly when there are many offspring in a clutch or litter - this is well known in altricial songbirds, for example). Perhaps the most elaborate food-related signal is the dance language of honeybees studied by Karl von Frisch. Young ravens signal to older, more experienced ravens when they come across new or untested food.

* Alarm calls: signals made in the presence of a threat from a predator, allowing all members of a social group (and often members of other species) to run for cover, become immobile, or gather into a group to reduce the risk of attack.

* Meta-communications: signals that modify the meaning of subsequent signals. The best known example is the play face in dogs,[citation needed] which signals that a subsequent aggressive signal is part of a play fight rather than a serious aggressive episode.
